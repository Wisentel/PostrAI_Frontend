
{
  "myResearch": [
    {
      "id": "1",
      "title": "Attention Is All You Need",
      "authors": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit"],
      "date": "1/4/2023",
      "labels": ["Natural Language Processing", "Few-Shot Learning", "Generative Models"],
      "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.",
      "summary": "The Transformer architecture has become a cornerstone of modern natural language processing. By replacing recurrent layers with multi-headed self-attention, the model can process input sequences in parallel, resulting in significantly faster training times.\n\nThe key innovation is the attention mechanism that allows the model to weigh the importance of different words in the input sequence when generating each output word. This approach has led to breakthrough performance in machine translation and has been adapted for numerous other NLP tasks.",
      "isStarred": true,
      "folder": "My Research Papers"
    },
    {
      "id": "2",
      "title": "BERT: Pre-training of Deep Bidirectional Transformers",
      "authors": ["Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova"],
      "date": "2/9/2023",
      "labels": ["Natural Language Processing", "Deep Learning", "Machine Learning"],
      "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers.",
      "summary": "BERT revolutionized natural language processing by introducing bidirectional training of transformers. This approach allows the model to understand context from both directions, leading to significant improvements in various NLP tasks including question answering, sentiment analysis, and named entity recognition.",
      "isStarred": false,
      "folder": "My Research Papers"
    }
  ],
  "privateCollection": [
    {
      "id": "3",
      "title": "GPT-3: Language Models are Few-Shot Learners",
      "authors": ["Tom B. Brown", "Benjamin Mann", "Nick Ryder", "Melanie Subbiah"],
      "date": "1/4/2023",
      "labels": ["Natural Language Processing", "Few-Shot Learning", "Generative Models"],
      "abstract": "We demonstrate that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model.",
      "summary": "GPT-3 demonstrates that very large language models can exhibit few-shot learning capabilities, meaning they can perform tasks with just a few examples, without requiring specific fine-tuning for each task.\n\nThe authors show that as model size increases, the gap between zero-shot, one-shot, and few-shot performance decreases, suggesting that large enough models can generalize to new tasks from just a few examples.\n\nThe paper discusses ethical considerations related to large language models and their potential impact on society, including issues of bias and misuse.",
      "isStarred": true,
      "folder": "Private Collection"
    },
    {
      "id": "4",
      "title": "Neural Machine Translation by Jointly Learning to Align and Translate",
      "authors": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"],
      "date": "15/3/2023",
      "labels": ["Machine Translation", "Neural Networks", "Deep Learning"],
      "abstract": "Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance.",
      "summary": "This paper introduces the attention mechanism in neural machine translation, which allows the model to focus on different parts of the input sequence when generating each word in the output sequence. This innovation significantly improved translation quality and became a foundational concept for subsequent developments in NLP.",
      "isStarred": false,
      "folder": "Private Collection"
    }
  ],
  "publicCollection": [
    {
      "id": "5",
      "title": "Deep Residual Learning for Image Recognition",
      "authors": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"],
      "date": "12/11/2022",
      "labels": ["Computer Vision", "Deep Learning", "Image Recognition"],
      "abstract": "Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions.",
      "summary": "ResNet introduced skip connections that allow gradients to flow directly through the network, enabling the training of much deeper networks. This innovation solved the vanishing gradient problem and made it possible to train networks with hundreds of layers, leading to significant improvements in image recognition tasks.",
      "isStarred": true,
      "folder": "Public Collection"
    },
    {
      "id": "6",
      "title": "Generative Adversarial Networks",
      "authors": ["Ian J. Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu"],
      "date": "8/6/2022",
      "labels": ["Machine Learning", "Generative Models", "Deep Learning"],
      "abstract": "We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G.",
      "summary": "GANs introduced a novel approach to generative modeling through adversarial training. The framework consists of two neural networks competing against each other, leading to the generation of highly realistic synthetic data. This has applications in image generation, data augmentation, and many other domains.",
      "isStarred": false,
      "folder": "Public Collection"
    }
  ]
}
